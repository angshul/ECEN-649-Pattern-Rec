{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import hashlib\n",
    "import glob\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(img: Image.Image):\n",
    "    return np.array(img).astype(np.float32) / 255.\n",
    "\n",
    "def integral_image(img: np.ndarray):\n",
    "    integral = np.cumsum(np.cumsum(img, axis=0), axis=1)\n",
    "    return np.pad(integral, (1, 1), 'constant', constant_values=(0, 0))[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"C:\\\\Users\\\\ANGSHUL\\\\Downloads\\\\dataset\"\n",
    "train_data_dir=os.path.join(data_dir,\"trainset\")\n",
    "train_faces=os.path.join(train_data_dir,\"faces\")\n",
    "train_non_faces=os.path.join(train_data_dir,\"non-faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_image_files = glob.glob(os.path.join(train_faces, '**', '*.png'), recursive=True)\n",
    "len(face_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_image_files=glob.glob(os.path.join(train_non_faces, '**', '*.png'), recursive=True)\n",
    "len(background_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def __call__(self, integral_image: np.ndarray) -> float:\n",
    "        try:\n",
    "            return np.sum(np.multiply(integral_image[self.y_pos, self.x_pos], self.coeffs))\n",
    "        except IndexError as e:\n",
    "            raise IndexError(str(e) + ' in ' + str(self))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(x={self.x}, y={self.y}, width={self.width}, height={self.height})' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this base class we can define our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature2h(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hw = width // 2\n",
    "        self.x_pos = [x,      x + hw,     x,          x + hw,\n",
    "                         x + hw, x + width,  x + hw,     x + width]\n",
    "        self.y_pos = [y,      y,          y + height, y + height,\n",
    "                         y,      y,          y + height, y + height]\n",
    "        self.coeffs   = [1,     -1,         -1,          1,\n",
    "                         -1,     1,          1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature2v(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hh = height // 2        \n",
    "        self.x_pos = [x,      x + width,  x,          x + width,\n",
    "                         x,      x + width,  x,          x + width]\n",
    "        self.y_pos = [y,      y,          y + hh,     y + hh,\n",
    "                         y + hh, y + hh,     y + height, y + height]\n",
    "        self.coeffs   = [-1,     1,          1,         -1,\n",
    "                         1,     -1,         -1,          1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature3h(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        tw = width // 3\n",
    "        self.x_pos = [x,        x + tw,    x,          x + tw,\n",
    "                         x + tw,   x + 2*tw,  x + tw,     x + 2*tw,\n",
    "                         x + 2*tw, x + width, x + 2*tw,   x + width]\n",
    "        self.y_pos = [y,        y,         y + height, y + height,\n",
    "                         y,        y,         y + height, y + height,\n",
    "                         y,        y,         y + height, y + height]\n",
    "        self.coeffs   = [-1,       1,         1,         -1,\n",
    "                          1,      -1,        -1,          1,\n",
    "                         -1,       1,         1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature3v(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        th = height // 3\n",
    "        self.x_pos = [x,        x + width,  x,          x + width,\n",
    "                         x,        x + width,  x,          x + width,\n",
    "                         x,        x + width,  x,          x + width]\n",
    "        self.y_pos = [y,        y,          y + th,     y + th,\n",
    "                         y + th,   y + th,     y + 2*th,   y + 2*th,\n",
    "                         y + 2*th, y + 2*th,   y + height, y + height]\n",
    "        self.coeffs   = [-1,        1,         1,         -1,\n",
    "                          1,       -1,        -1,          1,\n",
    "                         -1,        1,         1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature4(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hw = width // 2\n",
    "        hh = height // 2\n",
    "        self.x_pos = [x,      x + hw,     x,          x + hw,     # upper row\n",
    "                         x + hw, x + width,  x + hw,     x + width,\n",
    "                         x,      x + hw,     x,          x + hw,     # lower row\n",
    "                         x + hw, x + width,  x + hw,     x + width]\n",
    "        self.y_pos = [y,      y,          y + hh,     y + hh,     # upper row\n",
    "                         y,      y,          y + hh,     y + hh,\n",
    "                         y + hh, y + hh,     y + height, y + height, # lower row\n",
    "                         y + hh, y + hh,     y + height, y + height]\n",
    "        self.coeffs   = [1,     -1,         -1,          1,          # upper row\n",
    "                         -1,     1,          1,         -1,\n",
    "                         -1,     1,          1,         -1,          # lower row\n",
    "                          1,    -1,         -1,          1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = NamedTuple('Size', [('height', int), ('width', int)])\n",
    "Location = NamedTuple('Location', [('top', int), ('left', int)])\n",
    "\n",
    "\n",
    "def get_positions(base_shape: Size, window_size: int = WINDOW_SIZE):\n",
    "    return (Location(left=x, top=y)\n",
    "            for x in range(0, window_size-base_shape.width+1) \n",
    "            for y in range(0, window_size-base_shape.height+1))\n",
    "\n",
    "def get_shapes(base_shape: Size, window_size: int = WINDOW_SIZE):\n",
    "    base_height = base_shape.height\n",
    "    base_width = base_shape.width\n",
    "    return (Size(height=height, width=width)\n",
    "            for width in range(base_width, window_size + 1, base_width)\n",
    "            for height in range(base_height, window_size + 1, base_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantiate all possible shapes of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature2h features: 17100\n",
      "Number of feature2v features: 17100\n",
      "Number of feature3h features: 10830\n",
      "Number of feature3v features: 10830\n",
      "Number of feature4 features:  8100\n",
      "Total number of features:     63960\n"
     ]
    }
   ],
   "source": [
    "feature2h = list(Feature2h(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=1, width=2), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature2v = list(Feature2v(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=2, width=1), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature3h = list(Feature3h(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=1, width=3), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature3v = list(Feature3v(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=3, width=1), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature4  = list(Feature4(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=2, width=2), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "features = feature2h + feature2v + feature3h + feature3v + feature4\n",
    "\n",
    "print(f'Number of feature2h features: {len(feature2h)}')\n",
    "print(f'Number of feature2v features: {len(feature2v)}')\n",
    "print(f'Number of feature3h features: {len(feature3h)}')\n",
    "print(f'Number of feature3v features: {len(feature3v)}')\n",
    "print(f'Number of feature4 features:  {len(feature4)}')\n",
    "print(f'Total number of features:     {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(p: int, n: int, face_files, background_files):\n",
    "    xs = []\n",
    "    xs.extend([image_to_array(Image.open(f).convert('L')) for f in random.sample(face_image_files, p)])\n",
    "    xs.extend([image_to_array(Image.open(f).convert('L')) for f in np.random.choice(background_image_files, n, replace=True)])\n",
    "    ys = np.hstack([np.ones((p,)), np.zeros((n,))])\n",
    "    return np.array(xs), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 0.47778645157814026, standard deviation: 0.22844739258289337\n"
     ]
    }
   ],
   "source": [
    "image_samples, _ = build_data(499, 2000, face_image_files, background_image_files)\n",
    "\n",
    "sample_mean = image_samples.mean()\n",
    "sample_std = image_samples.std()\n",
    "del image_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(p: int, n: int, face_files, background_files, mean: float = sample_mean, std: float = sample_std):\n",
    "    xs, ys = build_data(p, n, face_files, background_files)\n",
    "    xs=(xs-mean)/std\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = normalize(499, 2000, face_image_files, background_image_files)\n",
    "\n",
    "xis = np.array([integral_image(x) for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = NamedTuple('Threshold', [('threshold', float), ('polarity', float)])\n",
    "\n",
    "ClassifierResult = NamedTuple('ClassifierResult', [('threshold', float), ('polarity', int), \n",
    "                                                   ('classification_error', float),\n",
    "                                                   ('classifier', Callable[[np.ndarray], float])])\n",
    "\n",
    "WeakClassifier = NamedTuple('WeakClassifier', [('threshold', float), ('polarity', int), \n",
    "                                               ('alpha', float), \n",
    "                                               ('classifier', Callable[[np.ndarray], float])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weak_classifier(x: np.ndarray, classifier: WeakClassifier) -> float:\n",
    "    polarity=classifier.polarity\n",
    "    theta=classifier.threshold\n",
    "    feature=classifier.classifier\n",
    "    return (np.sign((polarity * theta) - (polarity * feature(x))) + 1) // 2\n",
    "\n",
    "def create_strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
    "    sum_hypotheses = 0.\n",
    "    sum_alphas = 0.\n",
    "    for c in weak_classifiers:\n",
    "        sum_hypotheses += c.alpha * get_weak_classifier(x, c)\n",
    "        sum_alphas += c.alpha\n",
    "    return 1 if (sum_hypotheses >= .5*sum_alphas) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(w: np.ndarray) -> np.ndarray:\n",
    "    return w / w.sum()\n",
    "\n",
    "\n",
    "def get_threshold(ys: np.ndarray, ws: np.ndarray, zs: np.ndarray):  \n",
    "    # Sort according to score\n",
    "    p = np.argsort(zs)\n",
    "    zs, ys, ws = zs[p], ys[p], ws[p]\n",
    "    \n",
    "    # Determine the best threshold: build running sums\n",
    "    s_minus, s_plus = 0., 0.\n",
    "    t_minus, t_plus = 0., 0.\n",
    "    s_minuses, s_pluses = [], []\n",
    "    \n",
    "    for y, w in zip(ys, ws):\n",
    "        if y < .6:\n",
    "            s_minus += w\n",
    "            t_minus += w\n",
    "        else:\n",
    "            s_plus += w\n",
    "            t_plus += w\n",
    "        s_minuses.append(s_minus)\n",
    "        s_pluses.append(s_plus)\n",
    "    \n",
    "    # Determine the best threshold: select optimal threshold.\n",
    "    min_e = float('inf')\n",
    "    min_z=0 \n",
    "    polarity=0\n",
    "    for z, s_m, s_p in zip(zs, s_minuses, s_pluses):\n",
    "        error_1 = s_p + (t_minus - s_m)\n",
    "        error_2 = s_m + (t_plus - s_p)\n",
    "        if error_1 < min_e:\n",
    "            min_e = error_1\n",
    "            min_z = z\n",
    "            polarity = -1\n",
    "        elif error_2 < min_e:\n",
    "            min_e = error_2\n",
    "            min_z = z\n",
    "            polarity = 1\n",
    "    return Threshold(threshold=min_z, polarity=polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(f: Feature, xis: np.ndarray, ys: np.ndarray, ws: np.ndarray, parallel: Optional[Parallel] = None) -> ClassifierResult:   \n",
    "    if parallel is None:\n",
    "        parallel = Parallel(n_jobs=-1, backend='threading')\n",
    "    \n",
    "    # Determine all feature values\n",
    "    zs = np.array(parallel(delayed(f)(x) for x in xis))\n",
    "    \n",
    "    # Determine the best threshold\n",
    "    result = get_threshold(ys, ws, zs)\n",
    "            \n",
    "    # Determine the classification error\n",
    "    classification_error = 0.\n",
    "    for x, y, w in zip(xis, ys, ws):\n",
    "        h = (np.sign((result.polarity *result.threshold) - (result.polarity * f(x))) + 1) // 2\n",
    "        classification_error += w * np.abs(h - y)\n",
    "            \n",
    "    return ClassifierResult(threshold=result.threshold, polarity=result.polarity, \n",
    "                            classification_error=classification_error, classifier=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_check= 2000\n",
    "random_prob = 0.25\n",
    "\n",
    "def build_weak_classifiers(prefix: str, num_features: int, xis: np.ndarray, ys: np.ndarray, features: List[Feature], ws: Optional[np.ndarray] = None) -> Tuple[List[WeakClassifier], List[float]]:\n",
    "    if ws is None:\n",
    "        m = len(ys[ys < .5])  # number of negative example\n",
    "        l = len(ys[ys > .5])  # number of positive examples\n",
    "\n",
    "        # Initialize the weights\n",
    "        ws = np.zeros_like(ys)\n",
    "        ws[ys < .5] = 1./(2.*m)\n",
    "        ws[ys > .5] = 1./(2.*l)\n",
    "    \n",
    "    # Keep track of the history of the example weights.\n",
    "    w_history = [ws]\n",
    "\n",
    "    total_start_time = datetime.now()\n",
    "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
    "        weak_classifiers = []  # type: List[WeakClassifier]\n",
    "        for t in range(num_features):\n",
    "            print(f'Building weak classifier {t+1}/{num_features} ...')\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Normalize the weights\n",
    "            ws = normalize_weights(ws)\n",
    "            \n",
    "            status_counter = status_check\n",
    "\n",
    "            # Select best weak classifier for this round\n",
    "            best = ClassifierResult(polarity=0, threshold=0, classification_error=float('inf'), classifier=None)\n",
    "            for i, f in enumerate(features):\n",
    "                status_counter -= 1\n",
    "                improved = False\n",
    "\n",
    "                # Python runs singlethreaded. To speed things up,\n",
    "                # we're only anticipating every other feature, give or take.\n",
    "                if random_prob < 1.:\n",
    "                    skip_probability = np.random.random()\n",
    "                    if skip_probability > random_prob:\n",
    "                        continue\n",
    "\n",
    "                result = create_features(f, xis, ys, ws, parallel)\n",
    "                if result.classification_error < best.classification_error:\n",
    "                    improved = True\n",
    "                    best = result\n",
    "\n",
    "                # Print status every couple of iterations.\n",
    "                if improved or status_counter == 0:\n",
    "                    current_time = datetime.now()\n",
    "                    duration = current_time - start_time\n",
    "                    total_duration = current_time - total_start_time\n",
    "                    status_counter = status_check\n",
    "                    if improved:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated. Classification error improved to {best.classification_error:.5f} using {str(best.classifier)} ...')\n",
    "                    else:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated.')\n",
    "\n",
    "            # After the best classifier was found, determine alpha\n",
    "            beta = best.classification_error / (1 - best.classification_error)\n",
    "            alpha = np.log(1. / beta)\n",
    "            \n",
    "            # Build the weak classifier\n",
    "            classifier = WeakClassifier(threshold=best.threshold, polarity=best.polarity, classifier=best.classifier, alpha=alpha)\n",
    "            \n",
    "            # Update the weights for misclassified examples\n",
    "            for i, (x, y) in enumerate(zip(xis, ys)):\n",
    "                h = get_weak_classifier(x, classifier)\n",
    "                e = np.abs(h - y)\n",
    "                ws[i] = ws[i] * np.power(beta, 1-e)\n",
    "                \n",
    "            # Register this weak classifier           \n",
    "            weak_classifiers.append(classifier)\n",
    "            w_history.append(ws)\n",
    "        \n",
    "    \n",
    "    print(f'Done building {num_features} weak classifiers.')\n",
    "    return weak_classifiers, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building weak classifier 1/5 ...\n",
      "t=1/5 0.68s (0.68s in this stage) 5/63960 0.01% evaluated. Classification error improved to 0.29056 using Feature2h(x=0, y=4, width=2, height=1) ...\n",
      "t=1/5 23.00s (23.00s in this stage) 104/63960 0.16% evaluated. Classification error improved to 0.23315 using Feature2h(x=5, y=8, width=2, height=1) ...\n",
      "t=1/5 32.37s (32.37s in this stage) 158/63960 0.25% evaluated. Classification error improved to 0.20190 using Feature2h(x=8, y=5, width=2, height=1) ...\n",
      "t=1/5 93.43s (93.43s in this stage) 490/63960 0.76% evaluated. Classification error improved to 0.18037 using Feature2h(x=8, y=3, width=2, height=2) ...\n",
      "t=1/5 261.54s (261.54s in this stage) 1385/63960 2.16% evaluated. Classification error improved to 0.17734 using Feature2h(x=8, y=4, width=2, height=5) ...\n",
      "t=1/5 350.98s (350.98s in this stage) 1891/63960 2.95% evaluated. Classification error improved to 0.16758 using Feature2h(x=8, y=4, width=2, height=7) ...\n",
      "t=1/5 434.32s (434.32s in this stage) 2324/63960 3.63% evaluated. Classification error improved to 0.16410 using Feature2h(x=8, y=3, width=2, height=9) ...\n",
      "t=1/5 474.65s (474.65s in this stage) 2513/63960 3.93% evaluated. Classification error improved to 0.16233 using Feature2h(x=8, y=2, width=2, height=10) ...\n",
      "t=1/5 6492.27s (6492.27s in this stage) 34247/63960 53.54% evaluated. Classification error improved to 0.13029 using Feature3h(x=2, y=8, width=3, height=1) ...\n",
      "t=1/5 6529.26s (6529.26s in this stage) 34456/63960 53.87% evaluated. Classification error improved to 0.10453 using Feature3h(x=13, y=8, width=3, height=1) ...\n",
      "t=1/5 6536.48s (6536.48s in this stage) 34494/63960 53.93% evaluated. Classification error improved to 0.09478 using Feature3h(x=15, y=8, width=3, height=1) ...\n",
      "t=1/5 6599.24s (6599.24s in this stage) 34801/63960 54.41% evaluated. Classification error improved to 0.09379 using Feature3h(x=15, y=7, width=3, height=2) ...\n",
      "t=1/5 6646.80s (6646.80s in this stage) 35075/63960 54.84% evaluated. Classification error improved to 0.09304 using Feature3h(x=14, y=7, width=3, height=3) ...\n",
      "t=1/5 6649.02s (6649.02s in this stage) 35092/63960 54.86% evaluated. Classification error improved to 0.08729 using Feature3h(x=15, y=7, width=3, height=3) ...\n",
      "t=1/5 7082.74s (7082.74s in this stage) 37533/63960 58.68% evaluated. Classification error improved to 0.08679 using Feature3h(x=5, y=7, width=6, height=1) ...\n",
      "t=1/5 10468.68s (10468.68s in this stage) 55853/63960 87.32% evaluated. Classification error improved to 0.08679 using Feature3v(x=0, y=7, width=19, height=12) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-e47a16af2a40>:6: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"run_weak_classifier\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-33-e47a16af2a40> (8)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-e47a16af2a40>\", line 8:\u001b[0m\n",
      "\u001b[1mdef run_weak_classifier(x: np.ndarray, c: WeakClassifier) -> float:\n",
      "\u001b[1m    return weak_classifier(x=x, f=c.classifier, polarity=c.polarity, theta=c.threshold)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\redma\\Anaconda3\\lib\\site-packages\\numba\\compiler.py:742: NumbaWarning: \u001b[1mFunction \"run_weak_classifier\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-e47a16af2a40>\", line 7:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef run_weak_classifier(x: np.ndarray, c: WeakClassifier) -> float:\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  self.func_ir.loc))\n",
      "C:\\Users\\redma\\Anaconda3\\lib\\site-packages\\numba\\compiler.py:751: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-e47a16af2a40>\", line 7:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef run_weak_classifier(x: np.ndarray, c: WeakClassifier) -> float:\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building weak classifier 2/5 ...\n",
      "t=2/5 12013.84s (0.80s in this stage) 3/63960 0.00% evaluated. Classification error improved to 0.33800 using Feature2h(x=0, y=2, width=2, height=1) ...\n",
      "t=2/5 12014.68s (1.64s in this stage) 4/63960 0.00% evaluated. Classification error improved to 0.29656 using Feature2h(x=0, y=3, width=2, height=1) ...\n",
      "t=2/5 12016.30s (3.26s in this stage) 9/63960 0.01% evaluated. Classification error improved to 0.28542 using Feature2h(x=0, y=8, width=2, height=1) ...\n",
      "t=2/5 12031.94s (18.90s in this stage) 107/63960 0.17% evaluated. Classification error improved to 0.28504 using Feature2h(x=5, y=11, width=2, height=1) ...\n",
      "t=2/5 12045.26s (32.22s in this stage) 157/63960 0.24% evaluated. Classification error improved to 0.19752 using Feature2h(x=8, y=4, width=2, height=1) ...\n",
      "t=2/5 12118.43s (105.39s in this stage) 491/63960 0.77% evaluated. Classification error improved to 0.19033 using Feature2h(x=8, y=4, width=2, height=2) ...\n",
      "t=2/5 12392.41s (379.37s in this stage) 1891/63960 2.95% evaluated. Classification error improved to 0.18117 using Feature2h(x=8, y=4, width=2, height=7) ...\n",
      "t=2/5 12443.54s (430.50s in this stage) 2116/63960 3.31% evaluated. Classification error improved to 0.17631 using Feature2h(x=8, y=3, width=2, height=8) ...\n",
      "Building weak classifier 3/5 ...\n",
      "t=3/5 24651.62s (0.75s in this stage) 4/63960 0.00% evaluated. Classification error improved to 0.28520 using Feature2h(x=0, y=3, width=2, height=1) ...\n",
      "t=3/5 24697.00s (46.13s in this stage) 238/63960 0.37% evaluated. Classification error improved to 0.27452 using Feature2h(x=12, y=9, width=2, height=1) ...\n",
      "t=3/5 24723.95s (73.08s in this stage) 360/63960 0.56% evaluated. Classification error improved to 0.24265 using Feature2h(x=0, y=17, width=2, height=2) ...\n",
      "t=3/5 25066.94s (416.07s in this stage) 1946/63960 3.04% evaluated. Classification error improved to 0.24188 using Feature2h(x=12, y=7, width=2, height=7) ...\n",
      "t=3/5 25583.91s (933.04s in this stage) 4468/63960 6.98% evaluated. Classification error improved to 0.23494 using Feature2h(x=11, y=7, width=4, height=4) ...\n",
      "t=3/5 25584.66s (933.80s in this stage) 4469/63960 6.99% evaluated. Classification error improved to 0.21499 using Feature2h(x=11, y=8, width=4, height=4) ...\n",
      "t=3/5 28986.51s (4335.64s in this stage) 20604/63960 32.21% evaluated. Classification error improved to 0.21005 using Feature2v(x=9, y=11, width=3, height=2) ...\n",
      "t=3/5 29354.77s (4703.90s in this stage) 22476/63960 35.14% evaluated. Classification error improved to 0.20419 using Feature2v(x=14, y=3, width=4, height=4) ...\n",
      "t=3/5 29558.39s (4907.52s in this stage) 23538/63960 36.80% evaluated. Classification error improved to 0.20389 using Feature2v(x=7, y=11, width=5, height=2) ...\n",
      "Building weak classifier 4/5 ...\n",
      "t=4/5 37321.02s (0.86s in this stage) 5/63960 0.01% evaluated. Classification error improved to 0.34504 using Feature2h(x=0, y=4, width=2, height=1) ...\n",
      "t=4/5 37322.87s (2.71s in this stage) 28/63960 0.04% evaluated. Classification error improved to 0.34258 using Feature2h(x=1, y=8, width=2, height=1) ...\n",
      "t=4/5 37324.57s (4.41s in this stage) 35/63960 0.05% evaluated. Classification error improved to 0.32641 using Feature2h(x=1, y=15, width=2, height=1) ...\n",
      "t=4/5 37342.92s (22.76s in this stage) 148/63960 0.23% evaluated. Classification error improved to 0.30376 using Feature2h(x=7, y=14, width=2, height=1) ...\n",
      "t=4/5 37358.81s (38.65s in this stage) 238/63960 0.37% evaluated. Classification error improved to 0.27930 using Feature2h(x=12, y=9, width=2, height=1) ...\n",
      "t=4/5 37382.58s (62.43s in this stage) 334/63960 0.52% evaluated. Classification error improved to 0.26075 using Feature2h(x=17, y=10, width=2, height=1) ...\n",
      "t=4/5 37447.44s (127.28s in this stage) 675/63960 1.05% evaluated. Classification error improved to 0.24646 using Feature2h(x=0, y=8, width=2, height=3) ...\n",
      "t=4/5 37506.62s (186.47s in this stage) 980/63960 1.53% evaluated. Classification error improved to 0.24584 using Feature2h(x=0, y=7, width=2, height=4) ...\n",
      "t=4/5 38156.91s (836.75s in this stage) 4225/63960 6.60% evaluated. Classification error improved to 0.23533 using Feature2h(x=12, y=8, width=4, height=3) ...\n",
      "t=4/5 38557.14s (1236.98s in this stage) 6225/63960 9.73% evaluated.\n",
      "t=4/5 38651.63s (1331.47s in this stage) 6679/63960 10.44% evaluated. Classification error improved to 0.23128 using Feature2h(x=11, y=9, width=6, height=1) ...\n",
      "t=4/5 38938.02s (1617.86s in this stage) 8169/63960 12.77% evaluated. Classification error improved to 0.23124 using Feature2h(x=11, y=8, width=6, height=8) ...\n",
      "t=4/5 39320.45s (2000.29s in this stage) 9937/63960 15.53% evaluated. Classification error improved to 0.22440 using Feature2h(x=10, y=8, width=8, height=4) ...\n",
      "t=4/5 39729.11s (2408.95s in this stage) 11937/63960 18.66% evaluated.\n",
      "Building weak classifier 5/5 ...\n",
      "t=5/5 49815.94s (0.81s in this stage) 29/63960 0.04% evaluated. Classification error improved to 0.35524 using Feature2h(x=1, y=9, width=2, height=1) ...\n",
      "t=5/5 49818.70s (3.57s in this stage) 34/63960 0.05% evaluated. Classification error improved to 0.35295 using Feature2h(x=1, y=14, width=2, height=1) ...\n",
      "t=5/5 49819.49s (4.36s in this stage) 37/63960 0.06% evaluated. Classification error improved to 0.35150 using Feature2h(x=1, y=17, width=2, height=1) ...\n",
      "t=5/5 49827.46s (12.33s in this stage) 77/63960 0.12% evaluated. Classification error improved to 0.32290 using Feature2h(x=4, y=0, width=2, height=1) ...\n",
      "t=5/5 49829.04s (13.91s in this stage) 92/63960 0.14% evaluated. Classification error improved to 0.31198 using Feature2h(x=4, y=15, width=2, height=1) ...\n",
      "t=5/5 49874.80s (59.68s in this stage) 335/63960 0.52% evaluated. Classification error improved to 0.29972 using Feature2h(x=17, y=11, width=2, height=1) ...\n",
      "t=5/5 49878.04s (62.92s in this stage) 347/63960 0.54% evaluated. Classification error improved to 0.29334 using Feature2h(x=0, y=4, width=2, height=2) ...\n",
      "t=5/5 49990.82s (175.69s in this stage) 983/63960 1.54% evaluated. Classification error improved to 0.28672 using Feature2h(x=0, y=10, width=2, height=4) ...\n",
      "t=5/5 50146.57s (331.44s in this stage) 1778/63960 2.78% evaluated. Classification error improved to 0.28373 using Feature2h(x=17, y=9, width=2, height=6) ...\n",
      "t=5/5 50574.22s (759.09s in this stage) 3778/63960 5.91% evaluated.\n",
      "t=5/5 51218.77s (1403.64s in this stage) 6759/63960 10.57% evaluated. Classification error improved to 0.25697 using Feature2h(x=1, y=14, width=6, height=2) ...\n",
      "t=5/5 53247.33s (3432.20s in this stage) 17353/63960 27.13% evaluated. Classification error improved to 0.25654 using Feature2v(x=14, y=0, width=1, height=2) ...\n",
      "t=5/5 53570.32s (3755.19s in this stage) 18916/63960 29.57% evaluated. Classification error improved to 0.25469 using Feature2v(x=5, y=15, width=2, height=2) ...\n",
      "t=5/5 53600.78s (3785.65s in this stage) 19045/63960 29.77% evaluated. Classification error improved to 0.24606 using Feature2v(x=13, y=0, width=2, height=2) ...\n",
      "t=5/5 53968.45s (4153.32s in this stage) 21045/63960 32.90% evaluated.\n",
      "t=5/5 54529.37s (4714.24s in this stage) 23617/63960 36.92% evaluated. Classification error improved to 0.24164 using Feature2v(x=12, y=0, width=5, height=2) ...\n",
      "t=5/5 54536.71s (4721.59s in this stage) 23635/63960 36.95% evaluated. Classification error improved to 0.23405 using Feature2v(x=13, y=0, width=5, height=2) ...\n",
      "t=5/5 61628.63s (11813.50s in this stage) 59379/63960 92.84% evaluated. Classification error improved to 0.21987 using Feature4(x=12, y=14, width=6, height=4) ...\n",
      "Done building 5 weak classifiers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weak_classifiers_5, w_history = build_weak_classifiers('5 rounds', 5, xis, ys, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WeakClassifier(threshold=-34.94171142578125, polarity=1, alpha=2.3535240166187434, classifier=Feature3v(x=0, y=7, width=19, height=12)),\n",
       " WeakClassifier(threshold=-1.3389663696289062, polarity=1, alpha=1.5415739225706282, classifier=Feature2h(x=8, y=3, width=2, height=8)),\n",
       " WeakClassifier(threshold=0.9956474304199219, polarity=-1, alpha=1.3621383392395696, classifier=Feature2v(x=7, y=11, width=5, height=2)),\n",
       " WeakClassifier(threshold=-2.0256080627441406, polarity=1, alpha=1.240216739953939, classifier=Feature2h(x=10, y=8, width=8, height=4)),\n",
       " WeakClassifier(threshold=-1.0986175537109375, polarity=1, alpha=1.2664070651570087, classifier=Feature4(x=12, y=14, width=6, height=4))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_classifiers_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir=os.path.join(data_dir,\"testset\")\n",
    "test_faces=os.path.join(test_data_dir,\"faces\")\n",
    "test_non_faces=os.path.join(test_data_dir,\"non-faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_image_files_test = glob.glob(os.path.join(test_faces, '**', '*.png'), recursive=True)\n",
    "len(face_image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_image_files_test=glob.glob(os.path.join(test_non_faces, '**', '*.png'), recursive=True)\n",
    "len(background_image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 0.5171371698379517, standard deviation: 0.235338494181633\n"
     ]
    }
   ],
   "source": [
    "image_samples_test, _ = build_data(471, 2000, face_image_files_test, background_image_files_test)\n",
    "\n",
    "sample_mean = image_samples_test.mean()\n",
    "sample_std = image_samples_test.std()\n",
    "\n",
    "\n",
    "print(f'Sample mean: {sample_mean}, standard deviation: {sample_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs, test_ys = normalize(471, 2000,face_image_files_test, background_image_files_test)\n",
    "test_xis = np.array([integral_image(x) for x in test_xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = NamedTuple('PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)])\n",
    "\n",
    "def predict(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, predicted_scores]:\n",
    "    c = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = c.ravel()\n",
    "    return c, predicted_scores(tn=tn, fp=fp, fn=fn, tp=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-886b5b384fd2>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"strong_classifier\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-34-886b5b384fd2> (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 3:\u001b[0m\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "\u001b[1m    sum_hypotheses = 0.\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-34-886b5b384fd2>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"strong_classifier\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 5:\u001b[0m\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "    <source elided>\n",
      "    sum_alphas = 0.\n",
      "\u001b[1m    for c in weak_classifiers:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\redma\\Anaconda3\\lib\\site-packages\\numba\\compiler.py:742: NumbaWarning: \u001b[1mFunction \"strong_classifier\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 2:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  self.func_ir.loc))\n",
      "C:\\Users\\redma\\Anaconda3\\lib\\site-packages\\numba\\compiler.py:751: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 2:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc))\n",
      "<ipython-input-34-886b5b384fd2>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"strong_classifier\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-34-886b5b384fd2> (5)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 5:\u001b[0m\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "    <source elided>\n",
      "    sum_alphas = 0.\n",
      "\u001b[1m    for c in weak_classifiers:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\redma\\Anaconda3\\lib\\site-packages\\numba\\compiler.py:742: NumbaWarning: \u001b[1mFunction \"strong_classifier\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 5:\u001b[0m\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "    <source elided>\n",
      "    sum_alphas = 0.\n",
      "\u001b[1m    for c in weak_classifiers:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  self.func_ir.loc))\n",
      "C:\\Users\\redma\\Anaconda3\\lib\\site-packages\\numba\\compiler.py:751: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-34-886b5b384fd2>\", line 5:\u001b[0m\n",
      "\u001b[1mdef strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
      "    <source elided>\n",
      "    sum_alphas = 0.\n",
      "\u001b[1m    for c in weak_classifiers:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.83, accuracy 0.60, recall 0.25, false positive rate 0.05, false negative rate 0.75.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ys_strong = np.array([create_strong_classifier(x, weak_classifiers_5) for x in test_xis])\n",
    "c, s = predict(test_ys, ys_strong)\n",
    "\n",
    "sns.heatmap(c / c.sum(), cmap='YlGnBu', annot=True, square=True, fmt='.1%',\n",
    "            xticklabels=['Predicted negative', 'Predicted positive'], \n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title(f'Confusion matrix for the strong classifier (stage 2)');\n",
    "\n",
    "print(f'Precision {s.tp/(s.tp+s.fp):.2f}, accuracy {(s.tp+s.tn)/(s.tp+s.tn+s.fp+s.fn):.2f}, recall {s.tp/(s.tp+s.fn):.2f}, false positive rate {s.fp/(s.fp+s.tn):.2f}, false negative rate {s.fn/(s.tp+s.fn):.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
