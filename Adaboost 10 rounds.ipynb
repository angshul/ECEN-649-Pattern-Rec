{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import hashlib\n",
    "import glob\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(img: Image.Image):\n",
    "    return np.array(img).astype(np.float32) / 255.\n",
    "\n",
    "def integral_image(img: np.ndarray):\n",
    "    integral = np.cumsum(np.cumsum(img, axis=0), axis=1)\n",
    "    return np.pad(integral, (1, 1), 'constant', constant_values=(0, 0))[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"C:\\\\Users\\\\ANGSHUL\\\\Downloads\\\\dataset\"\n",
    "train_data_dir=os.path.join(data_dir,\"trainset\")\n",
    "train_faces=os.path.join(train_data_dir,\"faces\")\n",
    "train_non_faces=os.path.join(train_data_dir,\"non-faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_image_files = glob.glob(os.path.join(train_faces, '**', '*.png'), recursive=True)\n",
    "len(face_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_image_files=glob.glob(os.path.join(train_non_faces, '**', '*.png'), recursive=True)\n",
    "len(background_image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pictures in the data set are 19×19. While the standard samples in Viola-Jones paper are 24×24, we adjust the window size based on the size of pictures we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def __call__(self, integral_image: np.ndarray) -> float:\n",
    "        try:\n",
    "            return np.sum(np.multiply(integral_image[self.y_pos, self.x_pos], self.coeffs))\n",
    "        except IndexError as e:\n",
    "            raise IndexError(str(e) + ' in ' + str(self))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(x={self.x}, y={self.y}, width={self.width}, height={self.height})' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature2h(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hw = width // 2\n",
    "        self.x_pos = [x,      x + hw,     x,          x + hw,\n",
    "                         x + hw, x + width,  x + hw,     x + width]\n",
    "        self.y_pos = [y,      y,          y + height, y + height,\n",
    "                         y,      y,          y + height, y + height]\n",
    "        self.coeffs   = [1,     -1,         -1,          1,\n",
    "                         -1,     1,          1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature2v(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hh = height // 2        \n",
    "        self.x_pos = [x,      x + width,  x,          x + width,\n",
    "                         x,      x + width,  x,          x + width]\n",
    "        self.y_pos = [y,      y,          y + hh,     y + hh,\n",
    "                         y + hh, y + hh,     y + height, y + height]\n",
    "        self.coeffs   = [-1,     1,          1,         -1,\n",
    "                         1,     -1,         -1,          1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature3h(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        tw = width // 3\n",
    "        self.x_pos = [x,        x + tw,    x,          x + tw,\n",
    "                         x + tw,   x + 2*tw,  x + tw,     x + 2*tw,\n",
    "                         x + 2*tw, x + width, x + 2*tw,   x + width]\n",
    "        self.y_pos = [y,        y,         y + height, y + height,\n",
    "                         y,        y,         y + height, y + height,\n",
    "                         y,        y,         y + height, y + height]\n",
    "        self.coeffs   = [-1,       1,         1,         -1,\n",
    "                          1,      -1,        -1,          1,\n",
    "                         -1,       1,         1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature3v(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        th = height // 3\n",
    "        self.x_pos = [x,        x + width,  x,          x + width,\n",
    "                         x,        x + width,  x,          x + width,\n",
    "                         x,        x + width,  x,          x + width]\n",
    "        self.y_pos = [y,        y,          y + th,     y + th,\n",
    "                         y + th,   y + th,     y + 2*th,   y + 2*th,\n",
    "                         y + 2*th, y + 2*th,   y + height, y + height]\n",
    "        self.coeffs   = [-1,        1,         1,         -1,\n",
    "                          1,       -1,        -1,          1,\n",
    "                         -1,        1,         1,         -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature4(Feature):\n",
    "    def __init__(self, x: int, y: int, width: int, height: int):\n",
    "        super().__init__(x, y, width, height)\n",
    "        hw = width // 2\n",
    "        hh = height // 2\n",
    "        self.x_pos = [x,      x + hw,     x,          x + hw,     # upper row\n",
    "                         x + hw, x + width,  x + hw,     x + width,\n",
    "                         x,      x + hw,     x,          x + hw,     # lower row\n",
    "                         x + hw, x + width,  x + hw,     x + width]\n",
    "        self.y_pos = [y,      y,          y + hh,     y + hh,     # upper row\n",
    "                         y,      y,          y + hh,     y + hh,\n",
    "                         y + hh, y + hh,     y + height, y + height, # lower row\n",
    "                         y + hh, y + hh,     y + height, y + height]\n",
    "        self.coeffs   = [1,     -1,         -1,          1,          # upper row\n",
    "                         -1,     1,          1,         -1,\n",
    "                         -1,     1,          1,         -1,          # lower row\n",
    "                          1,    -1,         -1,          1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = NamedTuple('Size', [('height', int), ('width', int)])\n",
    "Location = NamedTuple('Location', [('top', int), ('left', int)])\n",
    "\n",
    "\n",
    "def get_positions(base_shape: Size, window_size: int = WINDOW_SIZE):\n",
    "    return (Location(left=x, top=y)\n",
    "            for x in range(0, window_size-base_shape.width+1) \n",
    "            for y in range(0, window_size-base_shape.height+1))\n",
    "\n",
    "def get_shapes(base_shape: Size, window_size: int = WINDOW_SIZE):\n",
    "    base_height = base_shape.height\n",
    "    base_width = base_shape.width\n",
    "    return (Size(height=height, width=width)\n",
    "            for width in range(base_width, window_size + 1, base_width)\n",
    "            for height in range(base_height, window_size + 1, base_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature2h features: 17100\n",
      "Number of feature2v features: 17100\n",
      "Number of feature3h features: 10830\n",
      "Number of feature3v features: 10830\n",
      "Number of feature4 features:  8100\n",
      "Total number of features:     63960\n"
     ]
    }
   ],
   "source": [
    "feature2h = list(Feature2h(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=1, width=2), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature2v = list(Feature2v(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=2, width=1), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature3h = list(Feature3h(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=1, width=3), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature3v = list(Feature3v(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=3, width=1), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "feature4  = list(Feature4(location.left, location.top, shape.width, shape.height)\n",
    "                 for shape in get_shapes(Size(height=2, width=2), WINDOW_SIZE)\n",
    "                 for location in get_positions(shape, WINDOW_SIZE))\n",
    "\n",
    "features = feature2h + feature2v + feature3h + feature3v + feature4\n",
    "\n",
    "print(f'Number of feature2h features: {len(feature2h)}')\n",
    "print(f'Number of feature2v features: {len(feature2v)}')\n",
    "print(f'Number of feature3h features: {len(feature3h)}')\n",
    "print(f'Number of feature3v features: {len(feature3v)}')\n",
    "print(f'Number of feature4 features:  {len(feature4)}')\n",
    "print(f'Total number of features:     {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(p: int, n: int, face_files, background_files):\n",
    "    xs = []\n",
    "    xs.extend([image_to_array(Image.open(f).convert('L')) for f in random.sample(face_image_files, p)])\n",
    "    xs.extend([image_to_array(Image.open(f).convert('L')) for f in np.random.choice(background_image_files, n, replace=True)])\n",
    "    ys = np.hstack([np.ones((p,)), np.zeros((n,))])\n",
    "    return np.array(xs), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_samples, _ = build_data(499, 2000, face_image_files, background_image_files)\n",
    "\n",
    "sample_mean = image_samples.mean()\n",
    "sample_std = image_samples.std()\n",
    "del image_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(p: int, n: int, face_files, background_files, mean: float = sample_mean, std: float = sample_std):\n",
    "    xs, ys = build_data(p, n, face_files, background_files)\n",
    "    xs=(xs-mean)/std\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = normalize(499, 2000, face_image_files, background_image_files)\n",
    "\n",
    "xis = np.array([integral_image(x) for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = NamedTuple('Threshold', [('threshold', float), ('polarity', float)])\n",
    "\n",
    "ClassifierResult = NamedTuple('ClassifierResult', [('threshold', float), ('polarity', int), \n",
    "                                                   ('classification_error', float),\n",
    "                                                   ('classifier', Callable[[np.ndarray], float])])\n",
    "\n",
    "WeakClassifier = NamedTuple('WeakClassifier', [('threshold', float), ('polarity', int), \n",
    "                                               ('alpha', float), \n",
    "                                               ('classifier', Callable[[np.ndarray], float])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weak_classifier(x: np.ndarray, classifier: WeakClassifier) -> float:\n",
    "    polarity=classifier.polarity\n",
    "    theta=classifier.threshold\n",
    "    feature=classifier.classifier\n",
    "    return (np.sign((polarity * theta) - (polarity * feature(x))) + 1) // 2\n",
    "\n",
    "def create_strong_classifier(x: np.ndarray, weak_classifiers: List[WeakClassifier]) -> int:\n",
    "    sum_hypotheses = 0.\n",
    "    sum_alphas = 0.\n",
    "    for c in weak_classifiers:\n",
    "        sum_hypotheses += c.alpha * get_weak_classifier(x, c)\n",
    "        sum_alphas += c.alpha\n",
    "    return 1 if (sum_hypotheses >= .5*sum_alphas) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_threshold(ys: np.ndarray, ws: np.ndarray, zs: np.ndarray):  \n",
    "    # Sort according to score\n",
    "    p = np.argsort(zs)\n",
    "    zs, ys, ws = zs[p], ys[p], ws[p]\n",
    "    \n",
    "    # Determine the best threshold: build running sums\n",
    "    s_minus, s_plus = 0., 0.\n",
    "    t_minus, t_plus = 0., 0.\n",
    "    s_minuses, s_pluses = [], []\n",
    "    \n",
    "    for y, w in zip(ys, ws):\n",
    "        if y < .5:\n",
    "            s_minus += w\n",
    "            t_minus += w\n",
    "        else:\n",
    "            s_plus += w\n",
    "            t_plus += w\n",
    "        s_minuses.append(s_minus)\n",
    "        s_pluses.append(s_plus)\n",
    "    \n",
    "    # Determine the best threshold: select optimal threshold.\n",
    "    min_e = float('inf')\n",
    "    min_z=0 \n",
    "    polarity=0\n",
    "    for z, s_m, s_p in zip(zs, s_minuses, s_pluses):\n",
    "        error_1 = s_p + (t_minus - s_m)\n",
    "        error_2 = s_m + (t_plus - s_p)\n",
    "        if error_1 < min_e:\n",
    "            min_e = error_1\n",
    "            min_z = z\n",
    "            polarity = -1\n",
    "        elif error_2 < min_e:\n",
    "            min_e = error_2\n",
    "            min_z = z\n",
    "            polarity = 1\n",
    "    return Threshold(threshold=min_z, polarity=polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(f: Feature, xis: np.ndarray, ys: np.ndarray, ws: np.ndarray, parallel: Optional[Parallel] = None) -> ClassifierResult:   \n",
    "    if parallel is None:\n",
    "        parallel = Parallel(n_jobs=-1, backend='threading')\n",
    "    \n",
    "    # Determine all feature values\n",
    "    zs = np.array(parallel(delayed(f)(x) for x in xis))\n",
    "    \n",
    "    # Determine the best threshold\n",
    "    result = get_threshold(ys, ws, zs)\n",
    "            \n",
    "    # Determine the classification error\n",
    "    classification_error = 0.\n",
    "    for x, y, w in zip(xis, ys, ws):\n",
    "        h = (np.sign((result.polarity *result.threshold) - (result.polarity * f(x))) + 1) // 2\n",
    "        classification_error += w * np.abs(h - y)\n",
    "            \n",
    "    return ClassifierResult(threshold=result.threshold, polarity=result.polarity, \n",
    "                            classification_error=classification_error, classifier=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(w: np.ndarray) -> np.ndarray:\n",
    "    return w / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_check= 2000\n",
    "random_prob = 0.25\n",
    "\n",
    "def build_weak_classifiers(prefix: str, num_features: int, xis: np.ndarray, ys: np.ndarray, features: List[Feature], ws: Optional[np.ndarray] = None) -> Tuple[List[WeakClassifier], List[float]]:\n",
    "    if ws is None:\n",
    "        m = len(ys[ys < .5])  # number of negative example\n",
    "        l = len(ys[ys > .5])  # number of positive examples\n",
    "\n",
    "        # Initialize the weights\n",
    "        ws = np.zeros_like(ys)\n",
    "        ws[ys < .5] = 1./(2.*m)\n",
    "        ws[ys > .5] = 1./(2.*l)\n",
    "    \n",
    "    # Keep track of the history of the example weights.\n",
    "    w_history = [ws]\n",
    "\n",
    "    total_start_time = datetime.now()\n",
    "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
    "        weak_classifiers = []  # type: List[WeakClassifier]\n",
    "        for t in range(num_features):\n",
    "            print(f'Building weak classifier {t+1}/{num_features} ...')\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Normalize the weights\n",
    "            ws = normalize_weights(ws)\n",
    "            \n",
    "            status_counter = status_check\n",
    "\n",
    "            # Select best weak classifier for this round\n",
    "            best = ClassifierResult(polarity=0, threshold=0, classification_error=float('inf'), classifier=None)\n",
    "            for i, f in enumerate(features):\n",
    "                status_counter -= 1\n",
    "                improved = False\n",
    "\n",
    "                # Python runs singlethreaded. To speed things up,\n",
    "                # we're only anticipating every other feature, give or take.\n",
    "                if random_prob < 1.:\n",
    "                    skip_probability = np.random.random()\n",
    "                    if skip_probability > random_prob:\n",
    "                        continue\n",
    "\n",
    "                result = create_features(f, xis, ys, ws, parallel)\n",
    "                if result.classification_error < best.classification_error:\n",
    "                    improved = True\n",
    "                    best = result\n",
    "\n",
    "                # Print status every couple of iterations.\n",
    "                if improved or status_counter == 0:\n",
    "                    current_time = datetime.now()\n",
    "                    duration = current_time - start_time\n",
    "                    total_duration = current_time - total_start_time\n",
    "                    status_counter = status_check\n",
    "                    if improved:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated. Classification error improved to {best.classification_error:.5f} using {str(best.classifier)} ...')\n",
    "                    else:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated.')\n",
    "\n",
    "            # After the best classifier was found, determine alpha\n",
    "            beta = best.classification_error / (1 - best.classification_error)\n",
    "            alpha = np.log(1. / beta)\n",
    "            \n",
    "            # Build the weak classifier\n",
    "            classifier = WeakClassifier(threshold=best.threshold, polarity=best.polarity, classifier=best.classifier, alpha=alpha)\n",
    "            \n",
    "            # Update the weights for misclassified examples\n",
    "            for i, (x, y) in enumerate(zip(xis, ys)):\n",
    "                h = get_weak_classifier(x, classifier)\n",
    "                e = np.abs(h - y)\n",
    "                ws[i] = ws[i] * np.power(beta, 1-e)\n",
    "                \n",
    "            # Register this weak classifier           \n",
    "            weak_classifiers.append(classifier)\n",
    "            w_history.append(ws)\n",
    "        \n",
    "    \n",
    "    print(f'Done building {num_features} weak classifiers.')\n",
    "    return weak_classifiers, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building weak classifier 1/10 ...\n",
      "t=1/10 0.08s (0.08s in this stage) 6/63960 0.01% evaluated. Classification error improved to 0.45500 using Feature2h(x=0, y=5, width=2, height=1) ...\n",
      "t=1/10 0.14s (0.14s in this stage) 7/63960 0.01% evaluated. Classification error improved to 0.40000 using Feature2h(x=0, y=6, width=2, height=1) ...\n",
      "t=1/10 0.28s (0.28s in this stage) 20/63960 0.03% evaluated. Classification error improved to 0.39500 using Feature2h(x=1, y=0, width=2, height=1) ...\n",
      "t=1/10 0.58s (0.58s in this stage) 40/63960 0.06% evaluated. Classification error improved to 0.38500 using Feature2h(x=2, y=1, width=2, height=1) ...\n",
      "t=1/10 0.88s (0.88s in this stage) 52/63960 0.08% evaluated. Classification error improved to 0.38000 using Feature2h(x=2, y=13, width=2, height=1) ...\n",
      "t=1/10 1.20s (1.20s in this stage) 84/63960 0.13% evaluated. Classification error improved to 0.37000 using Feature2h(x=4, y=7, width=2, height=1) ...\n",
      "t=1/10 1.26s (1.26s in this stage) 86/63960 0.13% evaluated. Classification error improved to 0.36000 using Feature2h(x=4, y=9, width=2, height=1) ...\n",
      "t=1/10 1.50s (1.50s in this stage) 98/63960 0.15% evaluated. Classification error improved to 0.31000 using Feature2h(x=5, y=2, width=2, height=1) ...\n",
      "t=1/10 1.72s (1.72s in this stage) 116/63960 0.18% evaluated. Classification error improved to 0.28500 using Feature2h(x=6, y=1, width=2, height=1) ...\n",
      "t=1/10 3.01s (3.01s in this stage) 196/63960 0.30% evaluated. Classification error improved to 0.27000 using Feature2h(x=10, y=5, width=2, height=1) ...\n",
      "t=1/10 8.43s (8.43s in this stage) 523/63960 0.82% evaluated. Classification error improved to 0.24500 using Feature2h(x=10, y=0, width=2, height=2) ...\n",
      "t=1/10 8.49s (8.49s in this stage) 525/63960 0.82% evaluated. Classification error improved to 0.22000 using Feature2h(x=10, y=2, width=2, height=2) ...\n",
      "t=1/10 36.23s (36.23s in this stage) 2137/63960 3.34% evaluated. Classification error improved to 0.21500 using Feature2h(x=10, y=0, width=2, height=8) ...\n",
      "t=1/10 63.64s (63.64s in this stage) 3889/63960 6.08% evaluated. Classification error improved to 0.19500 using Feature2h(x=9, y=2, width=4, height=2) ...\n",
      "t=1/10 71.57s (71.57s in this stage) 4430/63960 6.92% evaluated. Classification error improved to 0.18500 using Feature2h(x=9, y=1, width=4, height=4) ...\n",
      "t=1/10 278.27s (278.27s in this stage) 17478/63960 27.32% evaluated. Classification error improved to 0.16500 using Feature2v(x=2, y=3, width=1, height=4) ...\n",
      "t=1/10 355.50s (355.50s in this stage) 22284/63960 34.84% evaluated. Classification error improved to 0.16000 using Feature2v(x=2, y=3, width=4, height=4) ...\n",
      "t=1/10 391.55s (391.55s in this stage) 24284/63960 37.97% evaluated.\n",
      "t=1/10 553.67s (553.66s in this stage) 34206/63960 53.48% evaluated. Classification error improved to 0.14000 using Feature3h(x=0, y=5, width=3, height=1) ...\n",
      "t=1/10 553.73s (553.73s in this stage) 34209/63960 53.48% evaluated. Classification error improved to 0.12000 using Feature3h(x=0, y=8, width=3, height=1) ...\n",
      "t=1/10 555.59s (555.58s in this stage) 34301/63960 53.63% evaluated. Classification error improved to 0.10500 using Feature3h(x=5, y=5, width=3, height=1) ...\n",
      "t=1/10 555.65s (555.65s in this stage) 34303/63960 53.63% evaluated. Classification error improved to 0.09500 using Feature3h(x=5, y=7, width=3, height=1) ...\n",
      "t=1/10 578.85s (578.85s in this stage) 35746/63960 55.89% evaluated. Classification error improved to 0.09000 using Feature3h(x=7, y=2, width=3, height=6) ...\n",
      "t=1/10 605.89s (605.89s in this stage) 37495/63960 58.62% evaluated. Classification error improved to 0.08500 using Feature3h(x=3, y=7, width=6, height=1) ...\n",
      "t=1/10 646.57s (646.56s in this stage) 40118/63960 62.72% evaluated. Classification error improved to 0.08000 using Feature3h(x=1, y=8, width=9, height=1) ...\n",
      "Building weak classifier 2/10 ...\n",
      "t=2/10 1013.68s (0.07s in this stage) 3/63960 0.00% evaluated. Classification error improved to 0.38859 using Feature2h(x=0, y=2, width=2, height=1) ...\n",
      "t=2/10 1013.81s (0.20s in this stage) 15/63960 0.02% evaluated. Classification error improved to 0.34918 using Feature2h(x=0, y=14, width=2, height=1) ...\n",
      "t=2/10 1014.27s (0.65s in this stage) 47/63960 0.07% evaluated. Classification error improved to 0.33424 using Feature2h(x=2, y=8, width=2, height=1) ...\n",
      "t=2/10 1014.33s (0.71s in this stage) 48/63960 0.07% evaluated. Classification error improved to 0.31658 using Feature2h(x=2, y=9, width=2, height=1) ...\n",
      "t=2/10 1015.24s (1.62s in this stage) 96/63960 0.15% evaluated. Classification error improved to 0.24457 using Feature2h(x=5, y=0, width=2, height=1) ...\n",
      "t=2/10 1016.59s (2.97s in this stage) 197/63960 0.31% evaluated. Classification error improved to 0.24049 using Feature2h(x=10, y=6, width=2, height=1) ...\n",
      "t=2/10 1021.85s (8.24s in this stage) 528/63960 0.82% evaluated. Classification error improved to 0.23505 using Feature2h(x=10, y=5, width=2, height=2) ...\n",
      "t=2/10 1029.83s (16.21s in this stage) 1086/63960 1.70% evaluated. Classification error improved to 0.23370 using Feature2h(x=7, y=1, width=2, height=4) ...\n",
      "t=2/10 1030.41s (16.80s in this stage) 1137/63960 1.78% evaluated. Classification error improved to 0.22147 using Feature2h(x=10, y=4, width=2, height=4) ...\n",
      "t=2/10 1039.24s (25.63s in this stage) 1673/63960 2.61% evaluated. Classification error improved to 0.21060 using Feature2h(x=10, y=2, width=2, height=6) ...\n",
      "t=2/10 1046.33s (32.71s in this stage) 2139/63960 3.34% evaluated. Classification error improved to 0.19973 using Feature2h(x=10, y=2, width=2, height=8) ...\n",
      "t=2/10 1084.79s (71.18s in this stage) 4601/63960 7.19% evaluated. Classification error improved to 0.19565 using Feature2h(x=4, y=0, width=4, height=5) ...\n",
      "t=2/10 1341.72s (328.10s in this stage) 20704/63960 32.37% evaluated. Classification error improved to 0.16304 using Feature2v(x=15, y=3, width=3, height=2) ...\n",
      "t=2/10 1536.51s (522.89s in this stage) 33083/63960 51.72% evaluated. Classification error improved to 0.15897 using Feature2v(x=4, y=6, width=15, height=6) ...\n",
      "t=2/10 1543.21s (529.59s in this stage) 33485/63960 52.35% evaluated. Classification error improved to 0.15625 using Feature2v(x=3, y=6, width=16, height=6) ...\n",
      "t=2/10 1572.54s (558.92s in this stage) 35485/63960 55.48% evaluated.\n",
      "Building weak classifier 3/10 ...\n",
      "t=3/10 1987.13s (0.07s in this stage) 6/63960 0.01% evaluated. Classification error improved to 0.35411 using Feature2h(x=0, y=5, width=2, height=1) ...\n",
      "t=3/10 1987.26s (0.19s in this stage) 26/63960 0.04% evaluated. Classification error improved to 0.32383 using Feature2h(x=1, y=6, width=2, height=1) ...\n",
      "t=3/10 1987.57s (0.50s in this stage) 47/63960 0.07% evaluated. Classification error improved to 0.28309 using Feature2h(x=2, y=8, width=2, height=1) ...\n",
      "t=3/10 1988.19s (1.12s in this stage) 97/63960 0.15% evaluated. Classification error improved to 0.26232 using Feature2h(x=5, y=1, width=2, height=1) ...\n",
      "t=3/10 1989.26s (2.19s in this stage) 176/63960 0.27% evaluated. Classification error improved to 0.25395 using Feature2h(x=9, y=4, width=2, height=1) ...\n",
      "t=3/10 1989.81s (2.75s in this stage) 193/63960 0.30% evaluated. Classification error improved to 0.23929 using Feature2h(x=10, y=2, width=2, height=1) ...\n",
      "t=3/10 1989.94s (2.87s in this stage) 199/63960 0.31% evaluated. Classification error improved to 0.23784 using Feature2h(x=10, y=8, width=2, height=1) ...\n",
      "t=3/10 1991.95s (4.88s in this stage) 341/63960 0.53% evaluated. Classification error improved to 0.23366 using Feature2h(x=17, y=17, width=2, height=1) ...\n",
      "t=3/10 1995.20s (8.13s in this stage) 527/63960 0.82% evaluated. Classification error improved to 0.22657 using Feature2h(x=10, y=4, width=2, height=2) ...\n",
      "t=3/10 1997.27s (10.21s in this stage) 665/63960 1.04% evaluated. Classification error improved to 0.21626 using Feature2h(x=17, y=16, width=2, height=2) ...\n",
      "t=3/10 2000.22s (13.15s in this stage) 842/63960 1.31% evaluated. Classification error improved to 0.20628 using Feature2h(x=10, y=5, width=2, height=3) ...\n",
      "t=3/10 2016.67s (29.60s in this stage) 1917/63960 3.00% evaluated. Classification error improved to 0.18309 using Feature2h(x=10, y=4, width=2, height=7) ...\n",
      "t=3/10 2020.38s (33.31s in this stage) 2140/63960 3.34% evaluated. Classification error improved to 0.17987 using Feature2h(x=10, y=3, width=2, height=8) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=3/10 2053.22s (66.16s in this stage) 4140/63960 6.47% evaluated.\n",
      "Building weak classifier 4/10 ...\n",
      "t=4/10 2968.44s (0.06s in this stage) 2/63960 0.00% evaluated. Classification error improved to 0.38074 using Feature2h(x=0, y=1, width=2, height=1) ...\n",
      "t=4/10 2968.62s (0.25s in this stage) 17/63960 0.03% evaluated. Classification error improved to 0.37455 using Feature2h(x=0, y=16, width=2, height=1) ...\n",
      "t=4/10 2969.30s (0.92s in this stage) 91/63960 0.14% evaluated. Classification error improved to 0.32664 using Feature2h(x=4, y=14, width=2, height=1) ...\n",
      "t=4/10 2969.85s (1.48s in this stage) 130/63960 0.20% evaluated. Classification error improved to 0.30910 using Feature2h(x=6, y=15, width=2, height=1) ...\n",
      "t=4/10 2973.09s (4.71s in this stage) 332/63960 0.52% evaluated. Classification error improved to 0.28688 using Feature2h(x=17, y=8, width=2, height=1) ...\n",
      "t=4/10 2973.15s (4.78s in this stage) 334/63960 0.52% evaluated. Classification error improved to 0.27351 using Feature2h(x=17, y=10, width=2, height=1) ...\n",
      "t=4/10 2978.37s (9.99s in this stage) 662/63960 1.03% evaluated. Classification error improved to 0.26194 using Feature2h(x=17, y=13, width=2, height=2) ...\n",
      "t=4/10 2986.77s (18.39s in this stage) 1257/63960 1.96% evaluated. Classification error improved to 0.22798 using Feature2h(x=17, y=12, width=2, height=4) ...\n",
      "t=4/10 2993.33s (24.96s in this stage) 1780/63960 2.78% evaluated. Classification error improved to 0.22503 using Feature2h(x=17, y=11, width=2, height=6) ...\n",
      "t=4/10 2996.43s (28.06s in this stage) 2015/63960 3.15% evaluated. Classification error improved to 0.22169 using Feature2h(x=17, y=11, width=2, height=7) ...\n",
      "t=4/10 3111.45s (143.07s in this stage) 9368/63960 14.65% evaluated. Classification error improved to 0.21901 using Feature2h(x=1, y=1, width=8, height=2) ...\n",
      "t=4/10 3232.64s (264.26s in this stage) 17320/63960 27.08% evaluated. Classification error improved to 0.19522 using Feature2v(x=12, y=3, width=1, height=2) ...\n",
      "t=4/10 3328.90s (360.53s in this stage) 23602/63960 36.90% evaluated. Classification error improved to 0.17263 using Feature2v(x=11, y=3, width=5, height=2) ...\n",
      "t=4/10 3369.45s (401.08s in this stage) 26212/63960 40.98% evaluated. Classification error improved to 0.15702 using Feature2v(x=11, y=3, width=7, height=2) ...\n",
      "Building weak classifier 5/10 ...\n",
      "t=5/10 3937.67s (0.06s in this stage) 13/63960 0.02% evaluated. Classification error improved to 0.42413 using Feature2h(x=0, y=12, width=2, height=1) ...\n",
      "t=5/10 3937.74s (0.13s in this stage) 14/63960 0.02% evaluated. Classification error improved to 0.40401 using Feature2h(x=0, y=13, width=2, height=1) ...\n",
      "t=5/10 3937.86s (0.25s in this stage) 17/63960 0.03% evaluated. Classification error improved to 0.39153 using Feature2h(x=0, y=16, width=2, height=1) ...\n",
      "t=5/10 3938.04s (0.43s in this stage) 27/63960 0.04% evaluated. Classification error improved to 0.37483 using Feature2h(x=1, y=7, width=2, height=1) ...\n",
      "t=5/10 3938.35s (0.74s in this stage) 34/63960 0.05% evaluated. Classification error improved to 0.34555 using Feature2h(x=1, y=14, width=2, height=1) ...\n",
      "t=5/10 3939.16s (1.55s in this stage) 77/63960 0.12% evaluated. Classification error improved to 0.31767 using Feature2h(x=4, y=0, width=2, height=1) ...\n",
      "t=5/10 3939.41s (1.80s in this stage) 97/63960 0.15% evaluated. Classification error improved to 0.26569 using Feature2h(x=5, y=1, width=2, height=1) ...\n",
      "t=5/10 3942.98s (5.37s in this stage) 341/63960 0.53% evaluated. Classification error improved to 0.26300 using Feature2h(x=17, y=17, width=2, height=1) ...\n",
      "t=5/10 3953.39s (15.78s in this stage) 965/63960 1.51% evaluated. Classification error improved to 0.25365 using Feature2h(x=17, y=9, width=2, height=3) ...\n",
      "t=5/10 3955.87s (18.26s in this stage) 1124/63960 1.76% evaluated. Classification error improved to 0.25338 using Feature2h(x=9, y=7, width=2, height=4) ...\n",
      "t=5/10 3957.81s (20.20s in this stage) 1259/63960 1.97% evaluated. Classification error improved to 0.20481 using Feature2h(x=17, y=14, width=2, height=4) ...\n",
      "t=5/10 3973.79s (36.18s in this stage) 2231/63960 3.49% evaluated. Classification error improved to 0.18826 using Feature2h(x=17, y=10, width=2, height=8) ...\n",
      "t=5/10 4004.05s (66.44s in this stage) 4231/63960 6.61% evaluated.\n",
      "t=5/10 4089.63s (152.02s in this stage) 9583/63960 14.98% evaluated. Classification error improved to 0.16925 using Feature2h(x=1, y=1, width=8, height=3) ...\n",
      "t=5/10 4124.73s (187.12s in this stage) 11772/63960 18.40% evaluated. Classification error improved to 0.16532 using Feature2h(x=0, y=1, width=10, height=3) ...\n",
      "t=5/10 4498.19s (560.58s in this stage) 35759/63960 55.91% evaluated. Classification error improved to 0.16312 using Feature3h(x=8, y=1, width=3, height=6) ...\n",
      "t=5/10 4528.45s (590.84s in this stage) 37879/63960 59.22% evaluated. Classification error improved to 0.16109 using Feature3h(x=10, y=2, width=6, height=2) ...\n",
      "t=5/10 4535.76s (598.15s in this stage) 38332/63960 59.93% evaluated. Classification error improved to 0.14668 using Feature3h(x=9, y=1, width=6, height=4) ...\n",
      "t=5/10 4544.68s (607.07s in this stage) 38935/63960 60.87% evaluated. Classification error improved to 0.13268 using Feature3h(x=9, y=1, width=6, height=7) ...\n",
      "t=5/10 4578.43s (640.83s in this stage) 41139/63960 64.32% evaluated. Classification error improved to 0.13219 using Feature3h(x=8, y=1, width=9, height=6) ...\n",
      "t=5/10 4585.67s (648.06s in this stage) 41657/63960 65.13% evaluated. Classification error improved to 0.12592 using Feature3h(x=8, y=1, width=9, height=10) ...\n",
      "t=5/10 4586.95s (649.34s in this stage) 41759/63960 65.29% evaluated. Classification error improved to 0.12287 using Feature3h(x=8, y=1, width=9, height=11) ...\n",
      "Building weak classifier 6/10 ...\n",
      "t=6/10 4905.07s (0.06s in this stage) 8/63960 0.01% evaluated. Classification error improved to 0.32768 using Feature2h(x=0, y=7, width=2, height=1) ...\n",
      "t=6/10 4905.14s (0.12s in this stage) 10/63960 0.01% evaluated. Classification error improved to 0.31971 using Feature2h(x=0, y=9, width=2, height=1) ...\n",
      "t=6/10 4906.07s (1.06s in this stage) 56/63960 0.09% evaluated. Classification error improved to 0.31687 using Feature2h(x=2, y=17, width=2, height=1) ...\n",
      "t=6/10 4906.51s (1.50s in this stage) 78/63960 0.12% evaluated. Classification error improved to 0.25162 using Feature2h(x=4, y=1, width=2, height=1) ...\n",
      "t=6/10 4906.94s (1.93s in this stage) 97/63960 0.15% evaluated. Classification error improved to 0.22823 using Feature2h(x=5, y=1, width=2, height=1) ...\n",
      "t=6/10 4917.13s (12.12s in this stage) 752/63960 1.17% evaluated. Classification error improved to 0.21633 using Feature2h(x=5, y=0, width=2, height=3) ...\n",
      "t=6/10 4934.01s (28.99s in this stage) 1778/63960 2.78% evaluated. Classification error improved to 0.20341 using Feature2h(x=17, y=9, width=2, height=6) ...\n",
      "t=6/10 4978.82s (73.81s in this stage) 4668/63960 7.30% evaluated. Classification error improved to 0.19409 using Feature2h(x=8, y=7, width=4, height=5) ...\n",
      "t=6/10 5013.73s (108.72s in this stage) 7030/63960 10.99% evaluated. Classification error improved to 0.18694 using Feature2h(x=3, y=0, width=6, height=3) ...\n",
      "t=6/10 5055.59s (150.58s in this stage) 9785/63960 15.30% evaluated. Classification error improved to 0.17249 using Feature2h(x=1, y=0, width=8, height=4) ...\n",
      "t=6/10 5783.77s (878.76s in this stage) 57940/63960 90.59% evaluated. Classification error improved to 0.16376 using Feature4(x=10, y=11, width=4, height=4) ...\n",
      "Building weak classifier 7/10 ...\n",
      "t=7/10 5870.96s (0.06s in this stage) 4/63960 0.00% evaluated. Classification error improved to 0.35654 using Feature2h(x=0, y=3, width=2, height=1) ...\n",
      "t=7/10 5871.09s (0.19s in this stage) 9/63960 0.01% evaluated. Classification error improved to 0.31050 using Feature2h(x=0, y=8, width=2, height=1) ...\n",
      "t=7/10 5872.33s (1.43s in this stage) 76/63960 0.12% evaluated. Classification error improved to 0.30371 using Feature2h(x=3, y=18, width=2, height=1) ...\n",
      "t=7/10 5872.90s (2.00s in this stage) 97/63960 0.15% evaluated. Classification error improved to 0.28811 using Feature2h(x=5, y=1, width=2, height=1) ...\n",
      "t=7/10 5873.33s (2.44s in this stage) 115/63960 0.18% evaluated. Classification error improved to 0.28280 using Feature2h(x=6, y=0, width=2, height=1) ...\n",
      "t=7/10 5873.46s (2.56s in this stage) 125/63960 0.19% evaluated. Classification error improved to 0.26920 using Feature2h(x=6, y=10, width=2, height=1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=7/10 5873.84s (2.95s in this stage) 142/63960 0.22% evaluated. Classification error improved to 0.24741 using Feature2h(x=7, y=8, width=2, height=1) ...\n",
      "t=7/10 5874.53s (3.63s in this stage) 200/63960 0.31% evaluated. Classification error improved to 0.23354 using Feature2h(x=10, y=9, width=2, height=1) ...\n",
      "t=7/10 5895.21s (24.31s in this stage) 1526/63960 2.38% evaluated. Classification error improved to 0.22737 using Feature2h(x=17, y=10, width=2, height=5) ...\n",
      "t=7/10 5898.46s (27.56s in this stage) 1779/63960 2.78% evaluated. Classification error improved to 0.22261 using Feature2h(x=17, y=10, width=2, height=6) ...\n",
      "t=7/10 5902.75s (31.85s in this stage) 2014/63960 3.15% evaluated. Classification error improved to 0.22129 using Feature2h(x=17, y=10, width=2, height=7) ...\n",
      "t=7/10 5905.91s (35.01s in this stage) 2230/63960 3.48% evaluated. Classification error improved to 0.20538 using Feature2h(x=17, y=9, width=2, height=8) ...\n",
      "t=7/10 5975.47s (104.57s in this stage) 6782/63960 10.60% evaluated. Classification error improved to 0.17189 using Feature2h(x=3, y=1, width=6, height=2) ...\n",
      "t=7/10 6140.69s (269.79s in this stage) 17477/63960 27.32% evaluated. Classification error improved to 0.16863 using Feature2v(x=2, y=2, width=1, height=4) ...\n",
      "t=7/10 6165.20s (294.30s in this stage) 19186/63960 30.00% evaluated. Classification error improved to 0.15927 using Feature2v(x=3, y=3, width=2, height=4) ...\n",
      "t=7/10 6217.56s (346.66s in this stage) 22534/63960 35.23% evaluated. Classification error improved to 0.15677 using Feature2v(x=2, y=1, width=4, height=6) ...\n",
      "t=7/10 6237.06s (366.17s in this stage) 23690/63960 37.04% evaluated. Classification error improved to 0.14910 using Feature2v(x=1, y=3, width=5, height=4) ...\n",
      "Building weak classifier 8/10 ...\n",
      "t=8/10 6841.19s (0.06s in this stage) 1/63960 0.00% evaluated. Classification error improved to 0.41193 using Feature2h(x=0, y=0, width=2, height=1) ...\n",
      "t=8/10 6841.25s (0.12s in this stage) 6/63960 0.01% evaluated. Classification error improved to 0.38202 using Feature2h(x=0, y=5, width=2, height=1) ...\n",
      "t=8/10 6841.32s (0.19s in this stage) 11/63960 0.02% evaluated. Classification error improved to 0.35954 using Feature2h(x=0, y=10, width=2, height=1) ...\n",
      "t=8/10 6841.44s (0.31s in this stage) 26/63960 0.04% evaluated. Classification error improved to 0.32423 using Feature2h(x=1, y=6, width=2, height=1) ...\n",
      "t=8/10 6841.62s (0.49s in this stage) 55/63960 0.08% evaluated. Classification error improved to 0.29583 using Feature2h(x=2, y=16, width=2, height=1) ...\n",
      "t=8/10 6843.48s (2.36s in this stage) 167/63960 0.26% evaluated. Classification error improved to 0.29455 using Feature2h(x=8, y=14, width=2, height=1) ...\n",
      "t=8/10 6843.67s (2.55s in this stage) 181/63960 0.28% evaluated. Classification error improved to 0.28931 using Feature2h(x=9, y=9, width=2, height=1) ...\n",
      "t=8/10 6843.98s (2.86s in this stage) 197/63960 0.31% evaluated. Classification error improved to 0.26775 using Feature2h(x=10, y=6, width=2, height=1) ...\n",
      "t=8/10 6845.29s (4.16s in this stage) 259/63960 0.40% evaluated. Classification error improved to 0.25455 using Feature2h(x=13, y=11, width=2, height=1) ...\n",
      "t=8/10 6848.66s (7.54s in this stage) 525/63960 0.82% evaluated. Classification error improved to 0.24475 using Feature2h(x=10, y=2, width=2, height=2) ...\n",
      "t=8/10 6848.85s (7.73s in this stage) 531/63960 0.83% evaluated. Classification error improved to 0.23387 using Feature2h(x=10, y=8, width=2, height=2) ...\n",
      "t=8/10 6850.35s (9.22s in this stage) 604/63960 0.94% evaluated. Classification error improved to 0.23086 using Feature2h(x=14, y=9, width=2, height=2) ...\n",
      "t=8/10 6865.59s (24.46s in this stage) 1528/63960 2.39% evaluated. Classification error improved to 0.22975 using Feature2h(x=17, y=12, width=2, height=5) ...\n",
      "t=8/10 6867.95s (26.83s in this stage) 1673/63960 2.61% evaluated. Classification error improved to 0.22726 using Feature2h(x=10, y=2, width=2, height=6) ...\n",
      "t=8/10 6876.45s (35.33s in this stage) 2231/63960 3.49% evaluated. Classification error improved to 0.20463 using Feature2h(x=17, y=10, width=2, height=8) ...\n",
      "t=8/10 6951.45s (110.33s in this stage) 7031/63960 10.99% evaluated. Classification error improved to 0.19876 using Feature2h(x=3, y=1, width=6, height=3) ...\n",
      "t=8/10 7025.53s (184.40s in this stage) 11772/63960 18.40% evaluated. Classification error improved to 0.19780 using Feature2h(x=0, y=1, width=10, height=3) ...\n",
      "t=8/10 7161.14s (320.02s in this stage) 20588/63960 32.19% evaluated. Classification error improved to 0.19450 using Feature2v(x=8, y=13, width=3, height=2) ...\n",
      "Building weak classifier 9/10 ...\n",
      "t=9/10 7806.45s (0.06s in this stage) 1/63960 0.00% evaluated. Classification error improved to 0.31101 using Feature2h(x=0, y=0, width=2, height=1) ...\n",
      "t=9/10 7806.70s (0.31s in this stage) 8/63960 0.01% evaluated. Classification error improved to 0.29784 using Feature2h(x=0, y=7, width=2, height=1) ...\n",
      "t=9/10 7807.07s (0.68s in this stage) 28/63960 0.04% evaluated. Classification error improved to 0.29135 using Feature2h(x=1, y=8, width=2, height=1) ...\n",
      "t=9/10 7807.57s (1.18s in this stage) 64/63960 0.10% evaluated. Classification error improved to 0.26093 using Feature2h(x=3, y=6, width=2, height=1) ...\n",
      "t=9/10 7809.25s (2.86s in this stage) 179/63960 0.28% evaluated. Classification error improved to 0.22785 using Feature2h(x=9, y=7, width=2, height=1) ...\n",
      "t=9/10 7809.81s (3.42s in this stage) 198/63960 0.31% evaluated. Classification error improved to 0.22535 using Feature2h(x=10, y=7, width=2, height=1) ...\n",
      "t=9/10 7809.87s (3.48s in this stage) 199/63960 0.31% evaluated. Classification error improved to 0.18001 using Feature2h(x=10, y=8, width=2, height=1) ...\n",
      "t=9/10 7809.93s (3.54s in this stage) 200/63960 0.31% evaluated. Classification error improved to 0.17967 using Feature2h(x=10, y=9, width=2, height=1) ...\n",
      "t=9/10 7897.84s (91.45s in this stage) 5959/63960 9.32% evaluated. Classification error improved to 0.17632 using Feature2h(x=9, y=2, width=4, height=12) ...\n",
      "t=9/10 7916.92s (110.53s in this stage) 7265/63960 11.36% evaluated. Classification error improved to 0.17540 using Feature2h(x=3, y=0, width=6, height=4) ...\n",
      "t=9/10 7924.53s (118.14s in this stage) 7752/63960 12.12% evaluated. Classification error improved to 0.16927 using Feature2h(x=7, y=3, width=6, height=6) ...\n",
      "t=9/10 8392.86s (586.47s in this stage) 38333/63960 59.93% evaluated. Classification error improved to 0.16580 using Feature3h(x=9, y=2, width=6, height=4) ...\n",
      "t=9/10 8398.58s (592.19s in this stage) 38748/63960 60.58% evaluated. Classification error improved to 0.16506 using Feature3h(x=9, y=1, width=6, height=6) ...\n",
      "t=9/10 8401.45s (595.06s in this stage) 38934/63960 60.87% evaluated. Classification error improved to 0.16147 using Feature3h(x=9, y=0, width=6, height=7) ...\n",
      "t=9/10 8432.09s (625.70s in this stage) 40983/63960 64.07% evaluated. Classification error improved to 0.15254 using Feature3h(x=8, y=2, width=9, height=5) ...\n",
      "t=9/10 8440.09s (633.70s in this stage) 41544/63960 64.95% evaluated. Classification error improved to 0.14895 using Feature3h(x=8, y=1, width=9, height=9) ...\n",
      "t=9/10 8443.18s (636.79s in this stage) 41759/63960 65.29% evaluated. Classification error improved to 0.14731 using Feature3h(x=8, y=1, width=9, height=11) ...\n",
      "Building weak classifier 10/10 ...\n",
      "t=10/10 8777.03s (0.06s in this stage) 4/63960 0.00% evaluated. Classification error improved to 0.36630 using Feature2h(x=0, y=3, width=2, height=1) ...\n",
      "t=10/10 8777.10s (0.13s in this stage) 7/63960 0.01% evaluated. Classification error improved to 0.33947 using Feature2h(x=0, y=6, width=2, height=1) ...\n",
      "t=10/10 8777.28s (0.31s in this stage) 25/63960 0.04% evaluated. Classification error improved to 0.29204 using Feature2h(x=1, y=5, width=2, height=1) ...\n",
      "t=10/10 8777.84s (0.87s in this stage) 63/63960 0.10% evaluated. Classification error improved to 0.26104 using Feature2h(x=3, y=5, width=2, height=1) ...\n",
      "t=10/10 8777.91s (0.94s in this stage) 64/63960 0.10% evaluated. Classification error improved to 0.25276 using Feature2h(x=3, y=6, width=2, height=1) ...\n",
      "t=10/10 8778.77s (1.80s in this stage) 142/63960 0.22% evaluated. Classification error improved to 0.23767 using Feature2h(x=7, y=8, width=2, height=1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=10/10 8781.16s (4.19s in this stage) 292/63960 0.45% evaluated. Classification error improved to 0.20721 using Feature2h(x=15, y=6, width=2, height=1) ...\n",
      "t=10/10 8784.78s (7.82s in this stage) 530/63960 0.83% evaluated. Classification error improved to 0.20501 using Feature2h(x=10, y=7, width=2, height=2) ...\n",
      "t=10/10 8786.03s (9.06s in this stage) 619/63960 0.97% evaluated. Classification error improved to 0.19543 using Feature2h(x=15, y=6, width=2, height=2) ...\n",
      "t=10/10 8791.31s (14.34s in this stage) 966/63960 1.51% evaluated. Classification error improved to 0.18862 using Feature2h(x=17, y=10, width=2, height=3) ...\n",
      "t=10/10 8792.62s (15.66s in this stage) 1053/63960 1.64% evaluated. Classification error improved to 0.18811 using Feature2h(x=5, y=0, width=2, height=4) ...\n",
      "t=10/10 8800.49s (23.52s in this stage) 1525/63960 2.38% evaluated. Classification error improved to 0.18452 using Feature2h(x=17, y=9, width=2, height=5) ...\n",
      "t=10/10 8808.33s (31.36s in this stage) 2013/63960 3.15% evaluated. Classification error improved to 0.16950 using Feature2h(x=17, y=9, width=2, height=7) ...\n",
      "t=10/10 8814.48s (37.52s in this stage) 2428/63960 3.79% evaluated. Classification error improved to 0.15910 using Feature2h(x=17, y=8, width=2, height=9) ...\n",
      "t=10/10 8882.53s (105.56s in this stage) 7031/63960 10.99% evaluated. Classification error improved to 0.14745 using Feature2h(x=3, y=1, width=6, height=3) ...\n",
      "t=10/10 8913.34s (136.37s in this stage) 9031/63960 14.12% evaluated.\n",
      "t=10/10 8918.70s (141.73s in this stage) 9386/63960 14.67% evaluated. Classification error improved to 0.14145 using Feature2h(x=2, y=1, width=8, height=2) ...\n",
      "Done building 10 weak classifiers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weak_classifiers_10, w_history = build_weak_classifiers('10', 10, xis, ys, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WeakClassifier(threshold=-1.0794528722763062, polarity=1, alpha=2.442347035369204, classifier=Feature3h(x=1, y=8, width=9, height=1)),\n",
       " WeakClassifier(threshold=-1.4321107864379883, polarity=1, alpha=1.6863989535702277, classifier=Feature2v(x=3, y=6, width=16, height=6)),\n",
       " WeakClassifier(threshold=0.46091461181640625, polarity=-1, alpha=1.5172205278154027, classifier=Feature2h(x=10, y=3, width=2, height=8)),\n",
       " WeakClassifier(threshold=0.4938373565673828, polarity=-1, alpha=1.6805874370303648, classifier=Feature2v(x=11, y=3, width=7, height=2)),\n",
       " WeakClassifier(threshold=-18.68893814086914, polarity=1, alpha=1.9655459776808373, classifier=Feature3h(x=8, y=1, width=9, height=11)),\n",
       " WeakClassifier(threshold=-0.14815521240234375, polarity=1, alpha=1.6305413713154933, classifier=Feature4(x=10, y=11, width=4, height=4)),\n",
       " WeakClassifier(threshold=1.2345921993255615, polarity=-1, alpha=1.7417101568384277, classifier=Feature2v(x=1, y=3, width=5, height=4)),\n",
       " WeakClassifier(threshold=0.03292274475097656, polarity=-1, alpha=1.4210155286557582, classifier=Feature2v(x=8, y=13, width=3, height=2)),\n",
       " WeakClassifier(threshold=-17.948191046714783, polarity=1, alpha=1.755859708487424, classifier=Feature3h(x=8, y=1, width=9, height=11)),\n",
       " WeakClassifier(threshold=-0.5267586708068848, polarity=1, alpha=1.8033395932989547, classifier=Feature2h(x=2, y=1, width=8, height=2))]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_classifiers_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir=os.path.join(data_dir,\"testset\")\n",
    "test_faces=os.path.join(test_data_dir,\"faces\")\n",
    "test_non_faces=os.path.join(test_data_dir,\"non-faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_image_files_test = glob.glob(os.path.join(test_faces, '**', '*.png'), recursive=True)\n",
    "len(face_image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_image_files_test=glob.glob(os.path.join(test_non_faces, '**', '*.png'), recursive=True)\n",
    "len(background_image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 0.5260450839996338, standard deviation: 0.23823058605194092\n"
     ]
    }
   ],
   "source": [
    "image_samples_test, _ = build_data(471, 2000, face_image_files_test, background_image_files_test)\n",
    "\n",
    "sample_mean = image_samples_test.mean()\n",
    "sample_std = image_samples_test.std()\n",
    "\n",
    "\n",
    "print(f'Sample mean: {sample_mean}, standard deviation: {sample_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs, test_ys = normalize(471, 2000,face_image_files_test, background_image_files_test)\n",
    "test_xis = np.array([integral_image(x) for x in test_xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = NamedTuple('PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)])\n",
    "\n",
    "def predict(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, predicted_scores]:\n",
    "    c = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = c.ravel()\n",
    "    return c, predicted_scores(tn=tn, fp=fp, fn=fn, tp=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.71, accuracy 0.91, recall 0.92, false positive rate 0.09, false negative rate 0.08.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ys_strong = np.array([create_strong_classifier(x, weak_classifiers_10) for x in test_xis])\n",
    "c, s = predict(test_ys, ys_strong)\n",
    "\n",
    "sns.heatmap(c / c.sum(), cmap='YlGnBu', annot=True, square=True, fmt='.1%',\n",
    "            xticklabels=['Predicted negative', 'Predicted positive'], \n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title(f'Confusion matrix for the strong classifier (stage 2)');\n",
    "\n",
    "print(f'Precision {s.tp/(s.tp+s.fp):.2f}, accuracy {(s.tp+s.tn)/(s.tp+s.tn+s.fp+s.fn):.2f}, recall {s.tp/(s.tp+s.fn):.2f}, false positive rate {s.fp/(s.fp+s.tn):.2f}, false negative rate {s.fn/(s.tp+s.fn):.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
